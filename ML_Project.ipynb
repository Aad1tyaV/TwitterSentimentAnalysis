{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Aaditya Varshney, B225\n",
        "#Ram Srivastava, B224"
      ],
      "metadata": {
        "id": "Ui5hDoa6YlkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import sqlite3\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Load CSV file\n",
        "df_csv=pd.read_csv(\"Tweets.csv\")\n",
        "\n",
        "# Load SQLite database\n",
        "conn=sqlite3.connect(\"database.sqlite\")\n",
        "df_sqlite=pd.read_sql(\"SELECT * FROM Tweets;\",conn)\n",
        "\n",
        "# Combine datasets and remove duplicates\n",
        "df_combined=pd.concat([df_csv,df_sqlite]).drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "# Keep only relevant columns\n",
        "df_combined=df_combined[['text','airline_sentiment']].dropna()\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text=text.lower()\n",
        "    text=re.sub(r'http\\S+','',text)  # Remove URLs\n",
        "    text=re.sub(r'@\\w+', '',text)  # Remove mentions\n",
        "    text=re.sub(r'[^a-z\\s]','',text)  # Remove special characters\n",
        "    return text\n",
        "\n",
        "df_combined['cleaned_text']=df_combined['text'].astype(str).apply(preprocess_text)\n",
        "\n",
        "# Encode sentiment labels\n",
        "label_encoder=LabelEncoder()\n",
        "df_combined['sentiment_encoded']=label_encoder.fit_transform(df_combined['airline_sentiment'])\n",
        "\n",
        "# Split dataset\n",
        "X_train,X_test,y_train,y_test=train_test_split(\n",
        "    df_combined['cleaned_text'],df_combined['sentiment_encoded'],test_size=0.2,random_state=42\n",
        ")\n",
        "\n",
        "# Vectorize text using TF-IDF\n",
        "vectorizer=TfidfVectorizer(max_features=7000)\n",
        "X_train_tfidf=vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf=vectorizer.transform(X_test)\n",
        "\n",
        "# Define models\n",
        "rf_model=RandomForestClassifier(n_estimators=300,max_depth=50,random_state=42)\n",
        "xgb_model=XGBClassifier(n_estimators=300,learning_rate=0.1,max_depth=6,subsample=0.8,colsample_bytree=0.8,use_label_encoder=False,eval_metric='mlogloss')\n",
        "\n",
        "# Ensemble Voting Classifier\n",
        "ensemble_model=VotingClassifier(estimators=[\n",
        "    ('random_forest',rf_model),\n",
        "    ('xgboost',xgb_model)\n",
        "], voting='soft')\n",
        "\n",
        "# Train ensemble model\n",
        "ensemble_model.fit(X_train_tfidf,y_train)\n",
        "\n",
        "# Evaluate model\n",
        "y_pred=ensemble_model.predict(X_test_tfidf)\n",
        "report=classification_report(y_test,y_pred,target_names=label_encoder.classes_)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPsvoxDXlnqd",
        "outputId": "15183f82-d16e-45ab-a88f-c0d9db1259ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [09:45:17] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.84      0.98      0.90      3697\n",
            "     neutral       0.85      0.57      0.68      1204\n",
            "    positive       0.89      0.69      0.78       917\n",
            "\n",
            "    accuracy                           0.85      5818\n",
            "   macro avg       0.86      0.75      0.79      5818\n",
            "weighted avg       0.85      0.85      0.84      5818\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import sqlite3\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout, SpatialDropout1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load CSV file\n",
        "df_csv = pd.read_csv(\"Tweets.csv\")\n",
        "\n",
        "# Load SQLite database\n",
        "conn = sqlite3.connect(\"database.sqlite\")\n",
        "df_sqlite = pd.read_sql(\"SELECT * FROM Tweets;\", conn)\n",
        "\n",
        "# Combine datasets and remove duplicates\n",
        "df_combined = pd.concat([df_csv, df_sqlite]).drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "# Keep only relevant columns\n",
        "df_combined = df_combined[['text', 'airline_sentiment']].dropna()\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove special characters\n",
        "    return text\n",
        "\n",
        "df_combined['cleaned_text'] = df_combined['text'].astype(str).apply(preprocess_text)\n",
        "\n",
        "# Encode sentiment labels\n",
        "label_encoder = LabelEncoder()\n",
        "df_combined['sentiment_encoded'] = label_encoder.fit_transform(df_combined['airline_sentiment'])\n",
        "\n",
        "# Tokenization\n",
        "max_words = 10000  # Vocabulary size\n",
        "max_length = 100  # Max tweet length\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(df_combined['cleaned_text'])\n",
        "sequences = tokenizer.texts_to_sequences(df_combined['cleaned_text'])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df_combined['sentiment_encoded'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Build LSTM model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=128, input_length=max_length),\n",
        "    SpatialDropout1D(0.2),\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    Bidirectional(LSTM(32)),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(3, activation='softmax')  # 3 sentiment classes\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lenR8hfRrIYz",
        "outputId": "e5a4c678-a686-45bd-caef-540e66ecca8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 413ms/step - accuracy: 0.6854 - loss: 0.7539 - val_accuracy: 0.8340 - val_loss: 0.4319\n",
            "Epoch 2/5\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 407ms/step - accuracy: 0.8636 - loss: 0.3763 - val_accuracy: 0.8771 - val_loss: 0.3565\n",
            "Epoch 3/5\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 404ms/step - accuracy: 0.9146 - loss: 0.2472 - val_accuracy: 0.8964 - val_loss: 0.3247\n",
            "Epoch 4/5\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 404ms/step - accuracy: 0.9448 - loss: 0.1746 - val_accuracy: 0.9103 - val_loss: 0.3135\n",
            "Epoch 5/5\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 409ms/step - accuracy: 0.9564 - loss: 0.1331 - val_accuracy: 0.9144 - val_loss: 0.3089\n",
            "\u001b[1m182/182\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 59ms/step - accuracy: 0.9132 - loss: 0.3083\n",
            "Test Loss: 0.3088552951812744, Test Accuracy: 0.9144035577774048\n"
          ]
        }
      ]
    }
  ]
}